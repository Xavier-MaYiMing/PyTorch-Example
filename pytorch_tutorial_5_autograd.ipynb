{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d834bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff772ebd",
   "metadata": {},
   "source": [
    "$w$ and $b$ are parameters to be optimized. We need to compute the gradients w.r.t. those parameters. Therefore, \"requires_grad\" is set as True. A function that we apply to tensors to construcrt computational graph is an objective of class \"Function\". This object knows how to compute the function in both forward and backward directions. The backward propagation function is stored in \"grad_fn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3248faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradint function for z = <AddBackward0 object at 0x7fccb85c48d0>\n",
      "Gradint function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fccb85b7090>\n"
     ]
    }
   ],
   "source": [
    "print(f'Gradint function for z = {z.grad_fn}')\n",
    "print(f'Gradint function for loss = {loss.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b257c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2544, 0.2658, 0.1443],\n",
      "        [0.2544, 0.2658, 0.1443],\n",
      "        [0.2544, 0.2658, 0.1443],\n",
      "        [0.2544, 0.2658, 0.1443],\n",
      "        [0.2544, 0.2658, 0.1443]])\n",
      "tensor([0.2544, 0.2658, 0.1443])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d6d10",
   "metadata": {},
   "source": [
    "We can only perform gradient calculations using \"backward\" once on a given graph. If we need to do several backward calls on the same graph, we need to pass \"retain_graph=True\" to the \"backward\" call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd1c01f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Disabling gradient tracking\n",
    "# no_grad()\n",
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "    print(z.requires_grad)\n",
    "\n",
    "# detach()\n",
    "z = torch.matmul(x, w) + b\n",
    "z = z.detach()\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b9d21",
   "metadata": {},
   "source": [
    "The reasons to disable gradient tracking:\n",
    "1. Some parameters are frozen parameters.\n",
    "2. Speed up if only forward pass is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93b1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp + 1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada2a17",
   "metadata": {},
   "source": [
    "PyTorch accumulates the gradients!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
